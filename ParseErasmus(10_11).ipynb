{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links from Page 10:\n",
      "http://se4gd.eu/\n",
      "http://www.master-seas40.unina.it/\n",
      "https://secclo.eu/\n",
      "https://sinrem.eu/\n",
      "https://ssi-master.eu/\n",
      "http://www.emjmdsteps.eu/\n",
      "https://master-strains.eu/\n",
      "https://www.master-sucat.eu/\n",
      "http://www.emm-nano.org/\n",
      "https://mundusjournalism.com/\n",
      "https://www.tise-master.eu/\n",
      "http://emissmaster.omu.edu.tr/\n",
      "http://www.emhrpp.com/\n",
      "https://master-waves.eu/\n",
      "https://we-team.education/\n",
      "https://www.eusmat.net/international-studies/master/amase/\n",
      "http://www.nas.boku.ac.at/nuwi/emabg/\n",
      "https://www.eimas.uni-bayreuth.de/en/\n",
      "https://www.analyticalchemistry.eu/\n",
      "https://www.docnomads.eu/\n",
      "\n",
      "Links from Page 11:\n",
      "https://cosi-master.eu/\n",
      "https://www.master-cne.eu/\n",
      "https://www.cle.unibo.it/\n",
      "https://www.jointdegree.eu/de/circle-erasmus-mundus-international-masters-programme-on-circular-economy/\n",
      "https://erasmusmundus-ceeres.eu/\n",
      "https://master-bioref.eu\n",
      "https://master-biopham.eu/\n",
      "https://www.bioceb.eu/\n",
      "https://bimaplus.org/\n",
      "http://www.beinprecisionmedicine.org/\n",
      "https://bdma.ulb.ac.be/bdma/\n",
      "http://www.master-asc.org/\n",
      "http://www.afepa.eu\n",
      "http://www.4cities.eu\n"
     ]
    }
   ],
   "source": [
    "# Catalog URL\n",
    "base_url = \"https://www.eacea.ec.europa.eu/scholarships/erasmus-mundus-catalogue_en\"\n",
    "\n",
    "# Function to get the links from a specific page\n",
    "def get_program_links(page_number):\n",
    "    # Make the request to the Erasmus Mundus catalogue page (change the page number in the URL)\n",
    "    url = f\"{base_url}?page={page_number}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        program_links = soup.find_all('a', href=True)  # Finds all <a> tags with href attributes\n",
    "        links = [a['href'] for a in program_links if \"http\" in a['href'] and \"europa\" not in a['href']]  # Adjust filter if needed\n",
    "        return links\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page {page_number}\")\n",
    "        return []\n",
    "\n",
    "# Retrieve links from page 10 and 11\n",
    "page_10_links = get_program_links(9)\n",
    "page_11_links = get_program_links(10)\n",
    "\n",
    "# Print the retrieved links\n",
    "print(\"Links from Page 10:\")\n",
    "for link in page_10_links:\n",
    "    print(link)\n",
    "\n",
    "print(\"\\nLinks from Page 11:\")\n",
    "for link in page_11_links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_links = page_10_links+page_11_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: program_p1.json\n",
      "Saved: program_p2.json\n",
      "Saved: program_p3.json\n",
      "Saved: program_p4.json\n",
      "Saved: program_p5.json\n",
      "Skipping http://www.emjmdsteps.eu/ due to error: HTTPSConnectionPool(host='www.emjmdsteps.eu', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1020)')))\n",
      "Saved: program_p6.json\n",
      "Saved: program_p7.json\n",
      "Saved: program_p8.json\n",
      "Saved: program_p9.json\n",
      "Saved: program_p10.json\n",
      "Saved: program_p11.json\n",
      "Saved: program_p12.json\n",
      "Saved: program_p13.json\n",
      "Saved: program_p14.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/html/parser.py:171: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: program_p15.json\n",
      "Saved: program_p16.json\n",
      "Saved: program_p17.json\n",
      "Saved: program_p18.json\n",
      "Saved: program_p19.json\n",
      "Saved: program_p20.json\n",
      "Saved: program_p21.json\n",
      "Saved: program_p22.json\n",
      "Skipping https://www.jointdegree.eu/de/circle-erasmus-mundus-international-masters-programme-on-circular-economy/ due to error: 404 Client Error: Not Found for url: https://www.jointdegree.eu/de/circle-erasmus-mundus-international-masters-programme-on-circular-economy/\n",
      "Saved: program_p23.json\n",
      "Saved: program_p24.json\n",
      "Saved: program_p25.json\n",
      "Saved: program_p26.json\n",
      "Saved: program_p27.json\n",
      "Saved: program_p28.json\n",
      "Saved: program_p29.json\n",
      "Saved: program_p30.json\n",
      "Saved: program_p31.json\n",
      "Saved: program_p32.json\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "i = 1\n",
    "programs_json = {}\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "\n",
    "\n",
    "title_prefixes = (\"Erasmus Mundus Master\", \"International MSc\", \"International Master\")\n",
    "\n",
    "def extract_titles(soup):\n",
    "    \"\"\"Extracts all relevant titles from <title>, <h1>, <h2>, or <h3>.\"\"\"\n",
    "    titles = []\n",
    "\n",
    "    # Extract <title> tag text\n",
    "    title_tag = soup.find(\"title\")\n",
    "    if title_tag and title_tag.text.strip():\n",
    "        titles.append(title_tag.text.strip().replace('\\xa0',' ').replace('Home - ',''))\n",
    "\n",
    "    # Extract <h1>, <h2>, <h3> that start with the given prefixes\n",
    "    for tag in [\"h1\", \"h2\", \"h3\"]:\n",
    "        for heading in soup.find_all(tag):\n",
    "            heading_text = heading.text.strip()\n",
    "            for prefix in title_prefixes:\n",
    "                if prefix.lower() in heading_text.lower():                \n",
    "                    titles.append(heading_text.replace('\\xa0',' ').replace('Home - ',''))\n",
    "\n",
    "    return list(set(titles))  # Remove duplicates\n",
    "\n",
    "for link in program_links:\n",
    "    try:\n",
    "        # programs_json[f\"p{i}\"] = {}\n",
    "        # programs_json[f\"p{i}\"][\"url\"] = link\n",
    "        # programs_json[f\"p{i}\"][\"pages\"] = []\n",
    "\n",
    "        programs_json[f\"p{i}\"] = {\n",
    "            \"url\": link,\n",
    "            \"pages\": []\n",
    "        }\n",
    "\n",
    "        page = requests.get(link, headers=headers, timeout=10)  # Set timeout to avoid hanging\n",
    "        page.raise_for_status()  # Raise error for HTTP issues (e.g., 404, 500)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        program_titles = extract_titles(soup)\n",
    "        programs_json[f\"p{i}\"][\"titles\"] = program_titles\n",
    "\n",
    "        canonical_tag = soup.find(\"link\", rel=\"canonical\")\n",
    "        if canonical_tag and canonical_tag.get('href'):\n",
    "            program_base = urljoin(link, canonical_tag['href'])\n",
    "        else:\n",
    "            program_base = link  # Fallback to the original link if no canonical tag\n",
    "\n",
    "        programs_json[f\"p{i}\"][\"pages\"].append({\"url\": program_base,\n",
    "        \"html\":BeautifulSoup(requests.get(program_base, headers=headers).text, \"html.parser\").prettify()\n",
    "        })\n",
    "\n",
    "        sub_links = soup.find_all(\"a\", href=True)\n",
    "        \n",
    "        for sub_link in sub_links:\n",
    "            full_sub_link_url = urljoin(link, sub_link['href'])\n",
    "            # program_base_without_www = program_base.replace('www.', '')\n",
    "            parsed_url = urlparse(program_base)\n",
    "            domain = parsed_url.netloc\n",
    "            normalized_program_base = domain.replace('www.', '')\n",
    "            if normalized_program_base in full_sub_link_url and 'http' in full_sub_link_url and not any(d[\"url\"] == full_sub_link_url for d in programs_json[f\"p{i}\"][\"pages\"]):\n",
    "                programs_json[f\"p{i}\"][\"pages\"].append({\"url\": full_sub_link_url,         \n",
    "                \"html\": BeautifulSoup(requests.get(full_sub_link_url, headers=headers).text, \"html.parser\").prettify()\n",
    "        })\n",
    "\n",
    "        filename = f\"program_p{i}.json\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump( programs_json[f\"p{i}\"], f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Saved: {filename}\")  # Indicate progress\n",
    "\n",
    "    except (requests.RequestException, ValueError) as e:\n",
    "        print(f\"Skipping {link} due to error: {e}\")\n",
    "        continue  # Skip this site and move to the next one\n",
    "\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the programs_json to get all URLs from the \"pages\" key\n",
    "all_urls=[]\n",
    "\n",
    "for program_key, program_data in programs_json.items():\n",
    "    test=[]\n",
    "    test.append(program_data[\"titles\"])\n",
    "    # Extract URLs from the \"pages\" list\n",
    "    for page in program_data[\"pages\"]:\n",
    "        test.append(page[\"url\"])\n",
    "    all_urls.append(test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SE4GD – Software engineering with a purpose']\n",
      "['Master Seas 4.0 – Università degli Studi di Napoli Federico II']\n",
      "[]\n",
      "['SINReM – International Master of Science', 'International MSc in Sustainable and Innovative Natural Resource Management']\n",
      "['Joint International Master in Smart Systems Integrated Solutions', 'Joint International Master in Smart Systems Integrated Solutions (SSIs) - SSIs', 'Joint International Master in Smart Systems Integrated Solutions (SSIs)']\n",
      "['Master Erasmus Mundus STRAINS Université de Lille']\n",
      "['Erasmus Mundus Master in Sustainable Catalysis', 'Master SuCat – Université de Poitiers']\n",
      "['EMM Nano | Erasmus Mundus', 'Erasmus Mundus Master Nanoscience and Nanotechnology']\n",
      "[\"Erasmus Mundus Master's in Journalism, Media and Globalisation\"]\n",
      "['TISE']\n",
      "['The Erasmus Mundus Master in Soil Science (emiSS)']\n",
      "['Erasmus Mundus - Home']\n",
      "['ERASMUS MUNDUS MASTER WAVES', 'Master Waves | Home']\n",
      "['WE-TEAM']\n",
      "['AMASE - European School of Materials | EUSMAT | Saarland University']\n",
      "['European Master in Animal Biodiversity and Genomics – European Master in Animal Biodiversity and Genomics']\n",
      "['European Interdisciplinary Master African Studies']\n",
      "['Excellence in Analytical Chemistry']\n",
      "['Home']\n",
      "[]\n",
      "['Erasmus Mundus Master Chemical NanoEngineering', 'Master-cne – Erasmus Mundus Joint Master Degree']\n",
      "['Erasmus Mundus Master Course in European Literary Cultures', 'Erasmus Mundus Master Course in European Literary Cultures | Master CLE Erasmus Mundus']\n",
      "[]\n",
      "['Master Bioref']\n",
      "['Master BioPham Erasmus Mundus Université de Lille']\n",
      "['Bioceb - European Master in Biological and Chemical Engineering for a Sustainable Bioeconomy']\n",
      "['BIM A+ European Master in BIM - Building Information Modelling']\n",
      "[]\n",
      "['BDMA – BDMA – Big Data Management and Analytics']\n",
      "['ASC European Master']\n",
      "['AFEPA homepage — AFEPA']\n",
      "['4CITIES – Master Program in Urban Studies']\n"
     ]
    }
   ],
   "source": [
    "for i in all_urls:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
